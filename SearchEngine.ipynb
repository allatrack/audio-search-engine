{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Goal\n",
    "\n",
    "Our goal here is to build a personal audio search engine like «Shazam». \n",
    "\n",
    "Based on http://willdrevo.com/fingerprinting-and-audio-recognition-with-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Defining our Sound Descriptor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.mlab as mlab\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage.filters import maximum_filter\n",
    "from scipy.ndimage.morphology import (generate_binary_structure,\n",
    "                                      iterate_structure, binary_erosion)\n",
    "import hashlib\n",
    "from operator import itemgetter\n",
    "\n",
    "IDX_FREQ_I = 0\n",
    "IDX_TIME_J = 1\n",
    "\n",
    "######################################################################\n",
    "# Sampling rate, related to the Nyquist conditions, which affects\n",
    "# the range frequencies we can detect.\n",
    "DEFAULT_FS = 44100\n",
    "\n",
    "######################################################################\n",
    "# Size of the FFT window, affects frequency granularity\n",
    "DEFAULT_WINDOW_SIZE = 4096\n",
    "\n",
    "######################################################################\n",
    "# Ratio by which each sequential window overlaps the last and the\n",
    "# next window. Higher overlap will allow a higher granularity of offset\n",
    "# matching, but potentially more fingerprints.\n",
    "DEFAULT_OVERLAP_RATIO = 0.5\n",
    "\n",
    "######################################################################\n",
    "# Degree to which a fingerprint can be paired with its neighbors --\n",
    "# higher will cause more fingerprints, but potentially better accuracy.\n",
    "DEFAULT_FAN_VALUE = 15\n",
    "\n",
    "######################################################################\n",
    "# Minimum amplitude in spectrogram in order to be considered a peak.\n",
    "# This can be raised to reduce number of fingerprints, but can negatively\n",
    "# affect accuracy.\n",
    "DEFAULT_AMP_MIN = 10\n",
    "\n",
    "######################################################################\n",
    "# Number of cells around an amplitude peak in the spectrogram in order\n",
    "# for Dejavu to consider it a spectral peak. Higher values mean less\n",
    "# fingerprints and faster matching, but can potentially affect accuracy.\n",
    "PEAK_NEIGHBORHOOD_SIZE = 20\n",
    "\n",
    "######################################################################\n",
    "# Thresholds on how close or far fingerprints can be in time in order\n",
    "# to be paired as a fingerprint. If your max is too low, higher values of\n",
    "# DEFAULT_FAN_VALUE may not perform as expected.\n",
    "MIN_HASH_TIME_DELTA = 0\n",
    "MAX_HASH_TIME_DELTA = 200\n",
    "\n",
    "######################################################################\n",
    "# If True, will sort peaks temporally for fingerprinting;\n",
    "# not sorting will cut down number of fingerprints, but potentially\n",
    "# affect performance.\n",
    "PEAK_SORT = True\n",
    "\n",
    "######################################################################\n",
    "# Number of bits to throw away from the front of the SHA1 hash in the\n",
    "# fingerprint calculation. The more you throw away, the less storage, but\n",
    "# potentially higher collisions and misclassifications when identifying songs.\n",
    "FINGERPRINT_REDUCTION = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lib.wavio\n",
    "\n",
    "def fingerprint(channel_samples, Fs=DEFAULT_FS,\n",
    "                wsize=DEFAULT_WINDOW_SIZE,\n",
    "                wratio=DEFAULT_OVERLAP_RATIO,\n",
    "                fan_value=DEFAULT_FAN_VALUE,\n",
    "                amp_min=DEFAULT_AMP_MIN):\n",
    "    \"\"\"\n",
    "    FFT the channel, log transform output, find local maxima, then return\n",
    "    locally sensitive hashes.\n",
    "    \"\"\"\n",
    "    # FFT the signal and extract frequency components\n",
    "    arr2D = mlab.specgram(\n",
    "        channel_samples,\n",
    "        NFFT=wsize,\n",
    "        Fs=Fs,\n",
    "        window=mlab.window_hanning,\n",
    "        noverlap=int(wsize * wratio))[0]\n",
    "\n",
    "    # apply log transform since specgram() returns linear array\n",
    "    arr2D = 10 * np.log10(arr2D)\n",
    "    arr2D[arr2D == -np.inf] = 0  # replace infs with zeros\n",
    "\n",
    "    # find local maxima\n",
    "    local_maxima = get_2D_peaks(arr2D, plot=False, amp_min=amp_min)\n",
    "\n",
    "    # return hashes\n",
    "    return generate_hashes(list(local_maxima), fan_value=fan_value)\n",
    "\n",
    "\n",
    "def get_2D_peaks(arr2D, plot=False, amp_min=DEFAULT_AMP_MIN):\n",
    "    # http://docs.scipy.org/doc/scipy/reference/generated/scipy.ndimage.morphology.iterate_structure.html#scipy.ndimage.morphology.iterate_structure\n",
    "    struct = generate_binary_structure(2, 1)\n",
    "    neighborhood = iterate_structure(struct, PEAK_NEIGHBORHOOD_SIZE)\n",
    "\n",
    "    # find local maxima using our fliter shape\n",
    "    local_max = maximum_filter(arr2D, footprint=neighborhood) == arr2D\n",
    "    background = (arr2D == 0)\n",
    "    eroded_background = binary_erosion(background, structure=neighborhood,\n",
    "                                       border_value=1)\n",
    "\n",
    "    # Boolean mask of arr2D with True at peaks\n",
    "    detected_peaks = local_max - eroded_background\n",
    "\n",
    "    # extract peaks\n",
    "    amps = arr2D[detected_peaks]\n",
    "    j, i = np.where(detected_peaks)\n",
    "\n",
    "    # filter peaks\n",
    "    amps = amps.flatten()\n",
    "    peaks = zip(i, j, amps)\n",
    "    peaks_filtered = [x for x in peaks if x[2] > amp_min]  # freq, time, amp\n",
    "\n",
    "    # get indices for frequency and time\n",
    "    frequency_idx = [x[1] for x in peaks_filtered]\n",
    "    time_idx = [x[0] for x in peaks_filtered]\n",
    "\n",
    "    if plot:\n",
    "        # scatter of the peaks\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.imshow(arr2D)\n",
    "        ax.scatter(time_idx, frequency_idx)\n",
    "        ax.set_xlabel('Time')\n",
    "        ax.set_ylabel('Frequency')\n",
    "        ax.set_title(\"Spectrogram\")\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.show()\n",
    "\n",
    "    return zip(frequency_idx, time_idx)\n",
    "\n",
    "\n",
    "def generate_hashes(peaks, fan_value=DEFAULT_FAN_VALUE):\n",
    "    \"\"\"\n",
    "    Hash list structure:\n",
    "       sha1_hash[0:20]    time_offset\n",
    "    [(e05b341a9b77a51fd26, 32), ... ]\n",
    "    \"\"\"\n",
    "    if PEAK_SORT:\n",
    "        peaks.sort(key=lambda x: x[1])\n",
    "\n",
    "    for i in range(len(peaks)):\n",
    "        for j in range(1, fan_value):\n",
    "            if (i + j) < len(peaks):\n",
    "                \n",
    "                freq1 = peaks[i][IDX_FREQ_I]\n",
    "                freq2 = peaks[i + j][IDX_FREQ_I]\n",
    "                t1 = peaks[i][IDX_TIME_J]\n",
    "                t2 = peaks[i + j][IDX_TIME_J]\n",
    "                t_delta = t2 - t1\n",
    "\n",
    "                if t_delta >= MIN_HASH_TIME_DELTA and t_delta <= MAX_HASH_TIME_DELTA:\n",
    "                    h = hashlib.sha1(\"{}|{}|{}\".format(freq1, freq2, t_delta).encode('utf-8'))\n",
    "                    yield (h.hexdigest()[0:FINGERPRINT_REDUCTION], t1)\n",
    "\n",
    "\n",
    "def loadfile(filename):\n",
    "    # pydub does not support 24-bit wav files, use wavio when this occurs\n",
    "    try:\n",
    "        audiofile = AudioSegment.from_file(filename)\n",
    "\n",
    "        data = np.fromstring(audiofile._data, np.int16)\n",
    "\n",
    "        channels = []\n",
    "        for chn in range(audiofile.channels):\n",
    "            channels.append(data[chn::audiofile.channels])\n",
    "\n",
    "        fs = audiofile.frame_rate\n",
    "    except audioop.error:\n",
    "        fs, _, audiofile = wavio.readwav(filename)\n",
    "\n",
    "        audiofile = audiofile.T\n",
    "        audiofile = audiofile.astype(np.int16)\n",
    "\n",
    "        channels = []\n",
    "        for chn in audiofile:\n",
    "            channels.append(chn)\n",
    "    \n",
    "    return channels, fs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Extracting Features from Our Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/77 [00:00<?, ?it/s]\u001b[A/home/levabd/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:21: RuntimeWarning: divide by zero encountered in log10\n",
      "\n",
      "/home/levabd/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:43: DeprecationWarning: numpy boolean subtract, the `-` operator, is deprecated, use the bitwise_xor, the `^` operator, or the logical_xor function instead.\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pydub import AudioSegment\n",
    "from pydub.utils import audioop\n",
    "from hashlib import sha1\n",
    "\n",
    "# open the output index file for writing\n",
    "output = open(\"indexes/music.idx\", \"w\")\n",
    "\n",
    "music_index = pd.DataFrame(columns=('sound_id', 'hash', 'time_offset'))\n",
    "\n",
    "# use glob to grab the image paths and loop over them\n",
    "for musicPath in tqdm(glob.glob(\"dataset/*.mp3\")):\n",
    "    # extract the sound ID (i.e. the unique filename) from the sopund\n",
    "    # path and load the sound itself\n",
    "    soundID = musicPath[musicPath.rfind(\"/\") + 1:]\n",
    "    \n",
    "    channels, fs = loadfile(musicPath)\n",
    " \n",
    "    result = set()\n",
    "    channel_amount = len(channels)\n",
    "\n",
    "    for channeln, channel in enumerate(channels):\n",
    "        # TODO: Remove prints or change them into optional logging.\n",
    "        #print(\"Fingerprinting channel %d/%d for %s\" % (channeln + 1, channel_amount, musicPath))\n",
    "        hashes = set(fingerprint(channel, Fs=fs))\n",
    "        #print(\"Finished channel %d/%d for %s\" % (channeln + 1, channel_amount, musicPath))\n",
    "        \n",
    "        result |= hashes\n",
    "        for hash, offset in hashes:\n",
    "             music_index = music_index.append(\n",
    "                 {\n",
    "                     'sound_id': soundID,\n",
    "                     'hash': hash, \n",
    "                     'time_offset': offset\n",
    "                 }, ignore_index=True)\n",
    "    \n",
    "    features = [str(f) for f in result]\n",
    "    print(len(music_index))\n",
    "    output.write(\"%s,%s\\n\" % (soundID, \",\".join(features)))\n",
    "\n",
    "# close the index file\n",
    "output.close()\n",
    "\n",
    "print(\"Indexes saved with size(Bytes): {}\".format(os.stat('indexes/music.idx').st_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(music_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: The Searcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_matches():\n",
    "    \n",
    "\n",
    "def search(queryFile):\n",
    "    channels, fs = loadfile(musicPath)\n",
    "    \n",
    "    result = set()\n",
    "    channel_amount = len(channels)\n",
    "\n",
    "    for channeln, channel in enumerate(channels):\n",
    "        hashes = fingerprint(channel, Fs=fs)\n",
    "        result |= set(hashes)\n",
    "    \n",
    "    matches = []\n",
    "    for channel in channels:\n",
    "        matches.extend(find_matches(channel, fs))\n",
    "    return self.dejavu.align_matches(matches)\n",
    "    \n",
    "    return result\n",
    "    \n",
    "def chi2_distance(histA, histB, eps = 1e-10):\n",
    "    # compute the chi-squared distance\n",
    "    d = 0.5 * np.sum([((a - b) ** 2) / (a + b + eps) for (a, b) in zip(histA, histB)])\n",
    "    # return the chi-squared distance\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Performing a Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# load the query image and describe it\n",
    "query = cv2.imread(\"holidays/100000.jpg\")\n",
    "features = cd.describe(query)\n",
    " \n",
    "# perform the search\n",
    "searcher = Searcher('indexes/holidays.idx')\n",
    "results = searcher.search(features)\n",
    " \n",
    "# display the query\n",
    "print(\"Query\")\n",
    "plt.imshow(cv2.cvtColor(query, cv2.COLOR_BGR2RGB))\n",
    "plt.show()\n",
    " \n",
    "# loop over the results\n",
    "for (score, resultID) in results:\n",
    "    # load the result image and display it\n",
    "    result = cv2.imread(\"holidays/\" + resultID)\n",
    "    print(\"Result: {}\".format(\"holidays/\" + resultID))\n",
    "    plt.imshow(cv2.cvtColor(result, cv2.COLOR_BGR2RGB))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time 13 second. Not very quick, but simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# load the query image and describe it\n",
    "query = cv2.imread(\"holidays/110000.jpg\")\n",
    "features = cd.describe(query)\n",
    " \n",
    "# perform the search\n",
    "searcher = Searcher('indexes/holidays.idx')\n",
    "results = searcher.search(features)\n",
    " \n",
    "# display the query\n",
    "print(\"Query\")\n",
    "plt.imshow(cv2.cvtColor(query, cv2.COLOR_BGR2RGB))\n",
    "plt.show()\n",
    " \n",
    "# loop over the results\n",
    "for (score, resultID) in results:\n",
    "    # load the result image and display it\n",
    "    result = cv2.imread(\"holidays/\" + resultID)\n",
    "    print(\"Result\")\n",
    "    plt.imshow(cv2.cvtColor(result, cv2.COLOR_BGR2RGB))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
